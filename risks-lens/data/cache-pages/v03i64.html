<!DOCTYPE html>
<html lang="en" itemscope="" itemtype="http://schema.org/WebPage" prefix="og: http://ogp.me/ns#">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>The RISKS Digest Volume 3 Issue 64</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="apple-touch-icon-precomposed" sizes="57x57" href="/Risks/assets/favicons/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/Risks/assets/favicons/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/Risks/assets/favicons/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/Risks/assets/favicons/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="/Risks/assets/favicons/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="/Risks/assets/favicons/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="/Risks/assets/favicons/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/Risks/assets/favicons/apple-touch-icon-152x152.png">
<link rel="icon" type="image/png" href="/Risks/assets/favicons/favicon-196x196.png" sizes="196x196">
<link rel="icon" type="image/png" href="/Risks/assets/favicons/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/Risks/assets/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/Risks/assets/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/Risks/assets/favicons/favicon-128.png" sizes="128x128">
<meta name="application-name" content="The Risks Digest">
<meta name="msapplication-TileColor" content="#FFFFFF">
<meta name="msapplication-TileImage" content="/Risks/assets/favicons/mstile-144x144.png">
<meta name="msapplication-square70x70logo" content="/Risks/assets/favicons/mstile-70x70.png">
<meta name="msapplication-square150x150logo" content="/Risks/assets/favicons/mstile-150x150.png">
<meta name="msapplication-wide310x150logo" content="/Risks/assets/favicons/mstile-310x150.png">
<meta name="msapplication-square310x310logo" content="/Risks/assets/favicons/mstile-310x310.png">
<link rel="stylesheet" href="/Risks/assets/css/rlayout.min.css" media="screen">
<link rel="Up" href="/Risks/">
<link rel="Contents" href="/Risks/">
<link rel="Help" href="/Risks/">
<link rel="Home" href="/Risks/">
<link rel="Search" href="/Risks/search">
<link rel="First" href="/Risks/1/1">
<link rel="Last" href="/Risks/latest">
<link rel="Copyright" href="/Risks/info.html#cpr">
<link rel="alternate" href="/rdigest.rdf" type="application/rss+xml" title="RSS">
<link rel="alternate" href="/risksrss1.xml" type="application/rss+xml" title="RSS 1.0">
<link rel="alternate" href="/risksrss2" type="application/rss+xml" title="RSS 2.0">
<link rel="alternate" href="/risksatom.xml" type="application/atom+xml" title="ATOM">
<link rel="alternate" href="/risks.json" type="application/json" title="JSON Feed">
<link rel="alternate" href="/risksgorss1.xml" type="application/rss+xml" title="RSS 1.0 WAP">
<link rel="alternate" href="/risksgorss2" type="application/rss+xml" title="RSS 2.0 WAP">
<link rel="alternate" href="/risksgoatom.xml" type="application/atom+xml" title="ATOM WAP">
<link rel="alternate" href="/risksgo.json" type="application/json" title="JSON Feed">
<link rel="Index" href="/Risks/3/index">
<link rel="Prev" href="/Risks/3/63">
<link rel="Next" href="/Risks/3/65">
<meta name="DC.Creator" content="Peter G. Neumann">
<meta name="DC.Title" content="The RISKS Digest, Volume 3 Issue 64">
<meta name="DC.Language" content="en">
<meta name="DC.Type" content="Text">
<meta name="DC.Format" content="text/html">
<meta property="og:title" content="The RISKS Digest, Volume 3 Issue 64">
<meta property="og:site_name" content="The Risks Digest">
<meta property="og:locale" content="en">
<meta property="og:type" content="website">
<meta name="citation_author" content="Peter G. Neumann">
<meta name="citation_title" content="The RISKS Digest, Volume 3 Issue 64">
<meta name="citation_journal_title" content="The RISKS Digest, Volume 3 Issue 64">
<meta name="DC.Date" content="1986-09-24">
<meta property="og:article:published_time" content="1986-09-24">
<meta name="citation_volume" content="3">
<meta name="citation_issue" content="64">
<meta name="citation_publication_date" content="1986/09/24">
</head>
<body lang="en-GB">
            <nav><div class="btns">
                                                <a href="/Risks/3/63" title="Volume 3 Issue 63" itemprop="relatedLink"><i class="fad fa-fw fa-arrow-left"></i></a>
                                <a href="/Risks/3/index" title="Volume 3 Index" itemprop="relatedLink"><i class="fad fa-fw fa-list"></i></a>
                                                    <a href="/Risks/3/65" title="Volume 3 Issue 65" itemprop="relatedLink"><i class="fad fa-fw fa-arrow-right"></i></a>
                                        <a href="mailto:risks@csl.sri.com" title="Submit an article to RISKS" itemprop="relatedLink"><i class="fad fa-fw fa-envelope"></i></a>
            <a href="mailto:lindsay.marshall@newcastle.ac.uk?subject=Risks+Problem+3.64" title="Report a problem with this issue" itemprop="relatedLink"><i class="fad fa-fw fa-bug"></i></a>
            <a href="/Risks/archive/3/risks-3.64.gz" title="Fetch this issue as compressed plaintext" itemprop="relatedLink"><i class="fad fa-fw fa-cloud-download-alt"></i></a>
            <a href="/Risks/latest" title="The latest issue" itemprop="relatedLink"><i class="fad fa-fw fa-fast-forward"></i></a>
                            <a href="/Risks/" title="Information about RISKS" itemprop="relatedLink"><i class="fad fa-fw fa-info"></i></a>
                                </div>
        <div class="srch">
            <form action="/Risks/search" method="get">
                <label>Search RISKS <input type="text" name="query" placeholder="Query String"></label>
                <button type="submit"><i class="fad fa-fw fa-search"></i></button>
            </form>
        </div>
        <div class="clear"></div>
    </nav><h1 class="cntr" itemprop="name">
<span class="rdigest">The RISKS Digest</span><br>Volume 3 Issue 64</h1>
            <h2 class="cntr">Wednesday, 24th September 1986</h2>
        <h3 class="cntr">Forum on Risks to the Public in Computers and Related Systems</h3>
    <h3 class="cntr">
	<i><a href="//www.acm.org">ACM</a> Committee on Computers and Public Policy,
	<a href="http://www.csl.sri.com/neumann/neumann.html">Peter G. Neumann</a>, moderator</i>
        </h3>

    <h3>Contents</h3>
    

<dl>
<dt>
<img src="/Images/redball.gif" alt="o" width="14" height="14"><a href="#subj1">Sane sanity checks / risking public discussion</a>
</dt>
<dd>
<a href="#subj1.1">Jim Purtilo</a><br>
</dd>
<dt>
<img src="/Images/redball.gif" alt="o" width="14" height="14"><a href="#subj2">More (Maybe Too Much) On More Faults</a>
</dt>
<dd>
<a href="#subj2.1">Ken Dymond</a><br>
</dd>
<dt>
<img src="/Images/redball.gif" alt="o" width="14" height="14"><a href="#subj3">Re: Protection of personal information</a>
</dt>
<dd>
<a href="#subj3.1">Correction from David Chase</a><br>
</dd>
<dt>
<img src="/Images/redball.gif" alt="o" width="14" height="14"><a href="#subj4">Towards an effective definition of "autonomous" weapons</a>
</dt>
<dd>
<a href="#subj4.1">Herb Lin</a><br><a href="#subj4.2">Clifford Johnson [twice each]</a><br>
</dd>
<dt>
<img src="/Images/redball.gif" alt="o" width="14" height="14"><a href="/Risks">Info on RISKS (comp.risks)</a>
</dt>
</dl>
<a name="subj1" href="#subj1" id="subj1"><hr class="main"></a>
<h3>
<a name="subj1.1" href="#subj1.1" id="subj1.1"><img border="0" src="/Images/Misc/flash.gif" width="14" height="14" alt=""></a> Sane sanity checks / risking public discussion</h3>
<address>Jim Purtilo &lt;purtilo@brillig.umd.edu&gt;</address>
<i>Tue, 23 Sep 86 12:54:10 EDT</i>
<pre>
  [Regarding ``sanity checks'']

Let us remember that there are sane ``sanity checks'' as well as the other 
kind. About 8 years ago while a grad student at an Ohio university that 
probably ought to remain unnamed, I learned of the following follies:

The campus had long been doing class registration and scheduling via
computer, but the registrar insisted on a ``sanity check'' in the form of
hard copy.  Once each term, a dozen guys in overalls would spend the day
hauling a room full of paper boxes over from the CS center, representing a
paper copy of each document that had anything to do with the registration
process.  [I first took exception to this because their whole argument in
favor of "computerizing" was based on reduced costs, but I guess that should
be hashed out in NET.TREE-EATERS.]

No one in that registrar's office was at all interested in wading through
all that paper. Not even a little bit.

One fine day, the Burroughs people came through with a little upgrade to the
processor used by campus administration.  And some "unused status bits"
happened to float the other way.

This was right before the preregistration documents were run, and dutifully
about 12,000 students preregistration requests were scheduled and mailed
back to them.  All of them were signed up "PASS/FAIL".  This was
meticulously recorded on all those trees stored in the back room, but no one
wanted to look.

I suppose a moral would be ``if you include sanity checks, make sure a sane
person would be interested in looking at them.''


  [Regarding break-ins at Stanford]

A lot of the discussion seems to revolve about ``hey, Brian, you got what
you asked for'' (no matter how kindly it is phrased).  Without making
further editorial either way, I'd like to make sure that Brian is commended
for sharing the experience.  Sure would be a shame if ``coming clean'' about
a bad situation will be viewed as itself constituting a risk...

               [I am delighted to see this comment.  Thanks, Brian!  PGN]

</pre>
<a name="subj2" href="#subj2" id="subj2"><hr class="main"></a>
<h3>
<a name="subj2.1" href="#subj2.1" id="subj2.1"><img border="0" src="/Images/Misc/flash.gif" width="14" height="14" alt=""></a> More (Maybe Too Much) On More Faults</h3>
<address>"DYMOND, KEN" &lt;dymond@nbs-vms.ARPA&gt;</address>
<i>23 Sep 86 09:18:00 EDT</i>
<pre>
The intuitive sense made by Dave Benson's argument in <a href="/Risks/3/50">RISKS 3.50</a>, that

  &gt;We need to understand that the more faults found at any stage to
  &gt;engineering software the less confidence one has in the final product.  
  &gt;The more faults found, the higher the likelihood that faults remain.

seems to invite a search for confirming data because there are also counter-
intuitive possibilities.  For example there is the notion that the earlier
in the life cycle errors are detected, the cheaper to remedy them.  There is
a premium on finding faults early.  And the further notion that with tools
for writing requirements in some kind of formal language that can be checked
for syntactic and semantic completeness and consistency, it's possible to
detect at least some errors at requirements stage that may not have been
caught till later.  So SE projects using these and similar methods for other
stages in the life cycle would tend to show more errors earlier.  Would the
products from these projects be therefore less reliable than others made
with, say, more traditional, less careful, design and programming practice ?

Dave makes the further argument in <a href="/Risks/3/57">RISKS 3.57</a>:

  &gt;Certain models of software failure place increased "reliability" on
  &gt;software which has been exercised for long periods without fault. [...]

The models of software reliability exist to order our thinking about
reliability and to help predict behavior of software systems based on
observation of failure up to the current time.  The models that show
failures clustered early in time and then tapering off later do indeed model
an intuition but maybe not the one that more faults mean yet more faults.
Hence the need for data.  I suspect that the reality as shown by data, if it
exists, would be more complex than intuition allows.  More errors discovered
so far may just mean better software engineering methods.  As far as other
engineering fields, the failure vs time curve in manufactured products is
often taken to be tub-shaped, not exponentially decaying.  So more failures
are expected at the beginning and near the end of the useful life of a
"hard" engineered product.  Of course, "an unending sequence of irremediable
faults" should be the kiss of death for any product, whether from hard
engineering or soft.  But the trick is in knowing that the sequence is
unending.  The B-17, I seem to remember reading, had a rather rocky
development road in the 1930s, yet was not abandoned.  Was it just that the
aeronautical engineers at Boeing then had in mind some limit on the number
of faults and that this limit was not exceeded?  It might be easy to say in
hindsight.  On the other hand, sometimes foresight, in terms of spotting a
poor design at the outset, makes a difference, as in the only Chernobyl-type
power reactor outside the Soviet block.  It was bought by Finland (perhaps
this is what "Finlandization" means ?).  However the Finns also bought a
containment building from Westinghouse.

Ken Dymond

</pre>
<a name="subj3" href="#subj3" id="subj3"><hr class="main"></a>
<h3>
<a name="subj3.1" href="#subj3.1" id="subj3.1"><img border="0" src="/Images/Misc/flash.gif" width="14" height="14" alt=""></a> Re: Protection of personal information</h3>
<address>David Chase &lt;rbbb@rice.edu&gt;</address>
<i>Tue, 23 Sep 86 08:56:18 EDT</i>
<pre>
                       [The two participants requested this clarification 
                        be included for the record...  PGN]

You misinterpreted my message in a small way; I was writing about a
university attended by a friend, NOT Rice university.  To my knowledge, Rice
has been very good about protecting its students' privacy.  My student
number is NOT my social security number, though the university has that
number for good reasons.  I do not want anyone to think that I was talking
about Rice.       David

</pre>
<a name="subj4" href="#subj4" id="subj4"><hr class="main"></a>
<h3>
<a name="subj4.1" href="#subj4.1" id="subj4.1"><img border="0" src="/Images/Misc/flash.gif" width="14" height="14" alt=""></a> Towards an effective definition of "autonomous" weapons</h3>
<address>&lt;LIN@XX.LCS.MIT.EDU&gt;</address>
<i>Tue, 23 Sep 1986 18:00 EDT</i>
<pre>
         [THE FOLLOWING DISCOURSE INVOLVING CLIFF AND HERB IS LIKELY
          TO CONTINUE FOR A WHILE ON ARMS-D.  PLEASE RESPOND TO HERB LIN, 
          NOT TO RISKS ON THIS ONE.  HERB HAS VOLUNTEERED TO SUBMODERATE,
          AND THEN SUBMIT THE RESULTS TO RISKS.  PGN]

    From: Clifford Johnson &lt;GA.CJJ at Forsythe.Stanford.Edu&gt;

    An "autonomous weapon" [should be] defined to be any weapons system
    which is de facto preprogrammed to take decisions which, under the law 
    of nations, require the exercise of political or military discretion.

It's not a bad first attempt, and I think it is necessary to get a
handle on this.  With the realization that you have done us a service
in proposing your definition, let me comment on it.

I don't understand what it means for a weapon to "take a decision".  Clearly
you don't intend to include a depth charge set to explode at a certain
depth, and yet a depth charge could "decide" to explode at 100 feet given
certain input.

What I think you object to is the "preprogrammed" nature of a weapon,
in which a chip is giving arming, targeting and firing orders rather
than a human being.  What should be the role of the human being in
war?  I would think the most basic function is to decide what targets
should be attacked.  Thus, one modification to your definition is

    An "autonomous weapon" [should be] defined to be any weapons 
    system which is preprogrammed to SELECT targets.

This would include things like roving robot anti-tank jeeps, and
exclude the operation of LOW for the strategic forces.

But this definition would also exclude "fire-and-forget" weapons, and
I'm not sure I want to do that.  I want human DESIGNATION of a target
but I don't want the human being to remain exposed to enemy fire after
he has done so.  Thus, a second modification is 

    An "autonomous weapon" [should be] defined to be any weapons 
    system which is preprogrammed to SELECT targets in the absence of
    direct and immediate human intervention.

But then I note what a recent contributor said — MINES are autonomous
weapons, and I don't want to get rid of mines either, since I regard
mines as a defensive weapon par excellence.  Do I add mobility to the
definition?  I don't know.

</pre>
<hr>
<h3>
<a name="subj4.2" href="#subj4.2" id="subj4.2"><img border="0" src="/Images/Misc/flash.gif" width="14" height="14" alt=""></a> Towards an effective defintion of "autonomous" weapons</h3>
<address>Clifford Johnson &lt;GA.CJJ at Forsythe.Stanford.Edu&gt;</address>
<i>Monday, 22 September 1986 21:43-EDT</i>
<pre>
There's great difficulty in defining "autonomous weapons" so as to separate
some element that seems intuitively "horrible" about robot-decided death.
But a workable definition is necessary if, as CPSR tentatively proposes,
such weapons are to be declared illegal under international law, as have
chemical and nuclear weapons.  (Yes, the U.N. has declared even the
possession of nukes illegal, but it's not a binding provision.)

The problem is, of course, that many presently "acceptable" weapons already
indiscrminately-discriminate targets, e.g.  target-seeking munitions and
even passive mines.  Weapons kill, and civilians get killed too, that's war.
Is there an element exclusive to computerized weapons that is meaningful?

I don't have an answer, but feel the answer must be yes.  I proffer two
difficult lines of reasoning, derived from the philosophy of automatic
decisionmaking rather than extant weapon systems.  First, weapon control
systems that may automatically target-select among options based upon a
utility function (point score) that weighs killing people against destroying
hardware would seem especially unconscionable.  Second, but this presumes a
meaningful definition of "escalation," any weapons system that has the
capability to automatically escalate a conflict - and is conditionally
programmed to do so - would also seem unconscionable.

Into the first bracket would conceivably fall battle management software and
war games, into the second would fall war tools that in operation (de facto)
would take decisions which according to military regulations would otherwise
have required the exercise of discretion by a military commander or
politician.  The latter category would embrace booby-trap devices activated
in peacetime, such as mines and LOWCs; and here there is the precedent of
law which prohibits booby traps which threaten innocents in peacetime.
Perhaps the following "definition" could stand alone as *the* definition of
autonomous weapons to be banned:

An "autonomous weapon" is defined to be any weapons system which is
de facto preprogrammed to take decisions which, under the law of
nations, require the exercise of political or military discretion.

This might seem to beg the question, but it could be effective - military
manuals and international custom is often explicit on each commanders'
degree of authority/responsibility, and resolving whether a particular
weapon was autonomous would then be a CASE-BY-CASE DETERMINATION.  Note that
this could, and would, vary with the sphere of application of the weapons
system.  This is reasonable, just as there are circumstances in which
blockades or mining is "legal" and "illegal."

Of course, a case-in-point would be needed to launch the definition.
Obviously, I would propose that LOWCs were illegal.  How about battle
management software which decides to engage seemingly threatening entities
regardless of flag, in air or by sea?  Any other suggestions?  Does anyone
have any better ideas for a definition?

</pre>
<a name="subj5" href="#subj5" id="subj5"><hr class="main"></a>
<h3>
<a name="subj5.1" href="#subj5.1" id="subj5.1"><img border="0" src="/Images/Misc/flash.gif" width="14" height="14" alt=""></a> Towards an effective definition of "autonomous" weapons</h3>
<address>&lt;LIN@XX.LCS.MIT.EDU&gt;</address>
<i>Tue, 23 Sep 1986 18:09 EDT</i>
<pre>
In thinking about this question, I believe that ARMS-D and RISKS could
perform a real service to the defense community.  There is obviously a
concern among some ARMS-D and RISKS readers that autonomous weapons
are dangerous generically, and maybe they should be subject to some
legal restrictions.  Others are perhaps less opposed to the idea.

It is my own feeling that autonomous weapons could pose the same danger to
humanity that chemical or biological warfare pose, though they may be
militarily effective under certain circumstances.

I propose that the readership take up the questions posed by Cliff's recent
contribution:

    What is a good definition of an autonomous weapon?  

    What restrictions should be placed on autonomous weapons, and why?

    How might such limits be verified?

    Under what circumstances would autonomous weapons be militarily
    useful?

    Should we be pursuing such weapons at all?

    How close to production and deployment of such weapons are we?

Maybe a paper could be generated for publication?

</pre>

        <nav><div class="btns">
                                                <a href="/Risks/3/63" title="Volume 3 Issue 63" itemprop="relatedLink"><i class="fad fa-fw fa-arrow-left"></i></a>
                                <a href="/Risks/3/index" title="Volume 3 Index" itemprop="relatedLink"><i class="fad fa-fw fa-list"></i></a>
                                                    <a href="/Risks/3/65" title="Volume 3 Issue 65" itemprop="relatedLink"><i class="fad fa-fw fa-arrow-right"></i></a>
                                        <a href="mailto:risks@csl.sri.com" title="Submit an article to RISKS" itemprop="relatedLink"><i class="fad fa-fw fa-envelope"></i></a>
            <a href="mailto:lindsay.marshall@newcastle.ac.uk?subject=Risks+Problem+3.64" title="Report a problem with this issue" itemprop="relatedLink"><i class="fad fa-fw fa-bug"></i></a>
            <a href="/Risks/archive/3/risks-3.64.gz" title="Fetch this issue as compressed plaintext" itemprop="relatedLink"><i class="fad fa-fw fa-cloud-download-alt"></i></a>
            <a href="/Risks/latest" title="The latest issue" itemprop="relatedLink"><i class="fad fa-fw fa-fast-forward"></i></a>
                            <a href="/Risks/" title="Information about RISKS" itemprop="relatedLink"><i class="fad fa-fw fa-info"></i></a>
                                </div>
        <div class="srch">
            <form action="/Risks/search" method="get">
                <label>Search RISKS <input type="text" name="query" placeholder="Query String"></label>
                <button type="submit"><i class="fad fa-fw fa-search"></i></button>
            </form>
        </div>
        <div class="clear"></div>
    </nav><p class="problems">Please report problems with the web pages to <a href="mailto:lindsay.marshall@ncl.ac.uk?subject=Problem+with+RISKS+3.64">the maintainer</a></p>

    <div id="totop">
        <i class="fas fa-chevron-up"></i>Top
    </div>

    <script src="https://kit.fontawesome.com/f38fd5f4bf.js" crossorigin="anonymous"></script><script src="https://code.jquery.com/jquery-3.5.1.min.js"></script><script>
	var base = '/Risks';

	    function hostname(url)
    {
        var match = url.match(/:\/\/(www[0-9]?\.)?(.[^/:]+)/i);
        return match != null && match.length > 2 && typeof match[2] === 'string' && match[2].length > 0 ? match[2] : null;
    }

    function domain(url) {
        var host = hostname(url);
        
        if (host != null)
        {
            var parts = host.toLowerCase().split('.').reverse();
            
            if (parts != null && parts.length > 1)
            {
                for (var x in parts)
                {
                    if (parts[x].length > 3 || (parts[x].length == 3 && x > 0))
                    {
                        return parts[x];
                    }
                }
            }
        }
        return '';
    }

	$(function(){
	        $('.shield').on('click', function(e){
        e.preventDefault();
                    alert('Site privacy analysis in process of being implemented (and may never get released...)');
            });
	})
    </script><script src="/Risks/assets/js/risksmzr2.js"></script><script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script><script>
     WebFont.load({
        google: {
          families: ['Open Sans']
        }
      });
    </script>
</body>
</html>
