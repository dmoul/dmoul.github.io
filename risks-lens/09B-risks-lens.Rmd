<!-- 09B-risk-lens.Rmd -->
<!-- # part 2 -->

# Multiple perspectives

As the term ***n-gram*** implies, there are other word chunkings we could consider besides unigrams--and in how we determine the unigrams. Above I used `spaCy` to lemmatize words and treated these lemmas as the unigrams. In this section, we will look at unigraphs defined as [noun phrases](https://en.wikipedia.org/wiki/Noun_phrase) of one word. Noun phrases are one or more words that fulfill the function of a noun in a sentence. `spaCy` can parse with an awareness of the parts of speech, We'll look first at noun phrase unigrams ***from the titles*** of submissions, then ***from the body text*** of submissions. Following that we'll briefly look at bigrams determined by all of the above techniques.

<br>

## An example: "car"

Why do unigrams, bigrams and noun phrases provide different perspectives? Consider ***car***, which can stand alone as a unigram or be part of these longer n-grams (the trigrams each include a car-related bigram):

<br>

```{r gram-example-car}
ngram_example <- tribble(
  ~ngram,                      ~ngram_type,
  "car",                       "unigram, noun phrase unigram",
  "car door",                  "bigram, noun phrase bigram",
  "car electronics",           "bigram, noun phrase bigram",
  "car destroyed",             "bigram, noun phrase bigram",
  "car software",              "bigram, noun phrase bigram",
  "car computer system",       "trigram, noun phrase trigram",
  "car insurance policy",      "trigram, noun phrase trigram",
  "car lcd display",           "trigram, noun phrase trigram",
  "car navigational system",   "trigram, noun phrase trigram",
  "car repair shop",           "trigram, noun phrase trigram",
  "car train collision",       "trigram, noun phrase trigram",
  "car computer",              "bigram",
  "car insurance",             "bigram",
  "car lcd",                   "bigram",
  "car navigational",          "bigram",
  "car repair",                "bigram",
  "car train",                 "bigram"
) %>%
  mutate(n_words = str_count(ngram, " ") + 1)

ngram_example_with_counts <- tibble(
  type = c("unigram", "noun phrase unigram", "bigram", "noun phrase bigram", "trigram", "noun phrase trigram"),
  sort_order = 1:6,
  ngram = c("car",
           "car",
           glue_collapse(ngram_example %>% 
                           filter(n_words == 2) %>% pull(ngram), sep = ", "),
           glue_collapse(ngram_example %>% 
                           filter(n_words == 2 & str_detect(ngram_type, "noun phrase")) %>% pull(ngram), sep = ", "),
           glue_collapse(ngram_example %>% 
                           filter(n_words == 3) %>% pull(ngram), sep = ", "),
           glue_collapse(ngram_example %>% 
                           filter(n_words == 3 & str_detect(ngram_type, "noun phrase")) %>% pull(ngram), sep = ", ")
  ) 
) %>%
  mutate(n = str_count(ngram, ",") + 1)

ngram_example_with_counts$n[1] <- nrow(ngram_example)

ngram_example_with_counts %>%
  arrange(sort_order) %>%
  select(-sort_order) %>%
  filter(!str_detect(type, "noun|unigram")) %>%
  gt() %>%
  tab_options(
    table.width = pct(70)
  ) %>%
cols_label(
  type = md("***type***"),
  ngram = md("***ngram***"),
  n = md("***n***")
)

```

<br>

Unigrams are less precise than bigrams and trigrams, however unigrams are helpful in signaling a general topic. Counts of bigrams and trigrams are much lower than unigrams in the same corpus, which limits the possible analyses using n-grams where n > 1.

I find the n-grams in two ways:

1. Naively: simply count the occurrence of each n-gram by treating words in their base form ("lemma") as a unigram or bigram; and

2. With awareness of grammatical structure to identify noun phrases and categorize them by type. 

The results of the two approaches differ considerably, primarily because with noun phrases, a subset of a larger noun phrase is not counted as a shorter n-gram. Suppose each of the car n-grams above can be found in a distinct sentence of text. Then the count of each type are as follows:

<br>

```{r}
ngram_example_with_counts %>%
  arrange(sort_order) %>%
  select(-sort_order) %>%
  #filter(!str_detect(type, "noun")) %>%
  gt() %>%
  tab_options(
    table.width = pct(70)
  ) %>%
cols_label(
  type = md("***type***"),
  ngram = md("***ngram***"),
  n = md("***n***")
)

```

<br>

## Different perspectives: summary by type

These distinctions are evident in the twenty most common unigrams, bigrams and noun phrase unigrams. Note first the big difference in frequencies as seen in the x axis. The character of the lists is different too:

* Unigrams are for the most part general risk topics
* Noun phrase unigrams from titles are mostly organizations, countries and adjectives related to countries. 
noun phrase unigrams from body text are a wider mix of people, concepts, products, and organizations.
* Simple bigrams are more particular risk topics--or shards of them. For example, "credit card" and "buffer overflow" are sources of many risks, while "driving car" is likely part of a trigram, for example, "self driving car" and "autonomous driving car;" "air traffic" is probably part of "air traffic control."

<br>

```{r plot-top-unigrams-bigrams-faceted, fig.height=9, fig.width=9, fig.cap="Most common n-grams--all years"}

gram_counts <- bind_rows(
  d_unigrams_all %>%
    distinct(ngram, .keep_all = TRUE) %>%
    top_n(20, n) %>%
    select(ngram, n) %>%
    mutate(gram = "Simple unigrams from titles"),
  d_bigrams %>%
    distinct(ngram, .keep_all = TRUE) %>%
    top_n(20, n) %>%
    select(ngram, n) %>%
    mutate(gram = "Simple bigrams from titles"),
  d_noun_phrases %>%
    distinct(ngram, .keep_all = TRUE) %>%
    top_n(20, n)%>%
    select(ngram, n) %>%
    mutate(gram = "Noun phrase unigrams from titles"),
  d_noun_phrases_body %>%
    distinct(ngram, .keep_all = TRUE) %>%
    top_n(20, n)%>%
    select(ngram, n) %>%
    mutate(gram = "Noun phrase unigrams from body text")
) %>%
  mutate(gram = factor(gram, levels = c("Simple unigrams from titles", 
                                        "Simple bigrams from titles", 
                                        "Noun phrase unigrams from titles", 
                                        "Noun phrase unigrams from body text")))

gram_counts %>%
  ggplot(aes(n, reorder_within(ngram, n, gram))) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~gram, scales = "free") +
  scale_y_reordered() +
  theme(panel.grid.major.y = element_blank(),
            panel.grid.minor.y = element_blank()) +
  labs(title = "RISKS Digest: Most common unigrams and bigrams",
       subtitle = "All years",
       x = NULL, 
       y = NULL,
       caption = NULL)

```

<br>

## Unigrams from titles

The [Visualizations and discussion] in [The lens] uses lemmatized words as unigrams. Below we'll look at unigrams derrived from noun phrases, which provide a different perspective.

Counting the frequency of noun phrase unigrams in five-year periods and calculating a percentage of all mentions in each period makes visible some trends 

Common topics

* Aerospace has been a common source of topics, with references to manufacturers ***Airbus*** and ***Boeing*** as well as specific planes (***A320***, ***A380***), rockets (***Ariane***), and topics related to space exploration (***NASA***, ***Mariner***, ***mars***).
* Whereas ***privacy***, ***encryption*** and ***data*** are consistently in the top-five in the ranking of simple unigrams, here they are missing. Instead the list includes many more place-related terms: ***British***, ***Dutch***, ***German***, ***Japanese***, ***Russia*** and ***Russian***, ***China*** and ***Chinese***, ***Canadian***, ***American***, ***California***, ***San Francisco***, etc. We also see more organizations: ***congress***, ***IRS***, ***Stanford***, ***Intel***, ***Amazon***, ***Skype***, ***NSA***, and the same technology companies: ***Microsoft***, ***Facebook***, ***Google***, and ***Apple***.

Compare this to Figure \@ref(fig:unigrams-all-plot4)

```{r unigrams-noun-phrase-plot4-part2, fig.height=9, fig.width=9, fig.cap="Noun phrase unigram frequency: five-year periods"}

do_top_words_plot(df = d_noun_phrases %>%
                    filter(n_words == 1,
                             issue_year <= 2019), 
                  gram = "noun phrase unigrams",
                  which_plots = 4,
                  use_pct = TRUE)

```

<br>

## Unigrams from body text

There is a lot more body text than title text: `r round(n_body_words / n_title_words, 0)` times more. However, noun phrase unigrams do not include words that are not part of noun phrases or are part of multi-word noun phrases, so the set of words we have to work with is only about one tenth as large: (`r d_noun_phrases_body %>% filter(n_words == 1) %>% distinct(ngram) %>% nrow() %>% comma()` unique, noun phrase unigrams) compared to (`r d_unigrams_all %>% distinct(ngram) %>% nrow() %>% comma()` unique, simple unigrams. In the body text, the most common noun phrase is one word in length, as seen in Figure \@ref(fig:words-in-noun-phrase-bodytext):

```{r define-unique-noun-phrases-from-body-text}
d_unique_d_noun_phrases_body <- d_noun_phrases_body %>% 
  distinct(ngram, .keep_all = TRUE)

```

```{r words-in-noun-phrase-bodytext, fig.cap="Number of words in body text noun phrases"}
d_unique_d_noun_phrases_body %>%
  ggplot(aes(n_words)) +
  geom_histogram(binwidth = 1, fill = "steelblue", alpha = 0.7) +
  # scale_y_continuous(labels = label_number_si()) + 
  labs(title = "Number of words in body text noun phrases",
       subtitle = glue("{d_noun_phrases_body %>% distinct(ngram) %>% nrow()} distinct n-grams"),
       caption = my_caption)
  
```

<br>

Below are the most frequent noun phrase n-grams for each value of n in the submission body text.The 8-word n-gram is part of a frequent contributor's email signature ("uunet!..."). The 17-word ngram is most of a table in a call for papers for COMPASS '94 in [Volume 15 Issue 34](https://catless.ncl.ac.uk/Risks/15/34#subj13):

```
Papers, panel session proposals, tutorial proposals, and tools fair proposals are
solicited in the following areas:

Software Reliability     Software Safety     Computer Security
Formal Methods           Tools               Process Models
Real-Time Systems        Networks            Embedded Systems
V&V Practices            Certification       Standards
```

<br>

```{r}
d_unique_d_noun_phrases_body %>%
  arrange(n_words, desc(n)) %>%
  distinct(n_words, .keep_all = TRUE) %>%
  filter(n_words > 0) %>%
  select(n_words, ngram) %>%
  gt() %>%
  cols_label(
    n_words = md("***Number of words***"),
    ngram = md("***Most popular n-gram***")
  )
  

```

<br>

Looking at relative frequency of the ***noun phrase unigrams from body text*** within each time period in Figure \@ref(fig:unigrams-noun-phrase-body-plot4-part2), we can see the track of technological evolution: ***uucp***, ***arpanet***, ***clipper*** in the early years, ***java***, ***javascript*** and ***pgp*** in the middle years then ***siri***, ***bitcoin***, ***iphone*** and ***android*** in the later years. ***USB*** shows up in the last three five-year periods.

Compare this figure to Figure \@ref(fig:unigrams-all-plot4) and \@ref(fig:unigrams-noun-phrase-plot4-part2)

```{r unigrams-noun-phrase-body-plot4-part2, fig.height=9, fig.width=9, fig.cap="Noun phrase unigram frequency: five-year periods"}

do_top_words_plot(df = d_noun_phrases_body %>%
                    filter(n_words == 1,
                           issue_year <= 2019), 
                  gram = "noun phrase unigrams",
                  #max_words = n_words,
                  which_plots = 4,
                  use_pct = TRUE,
                  df_source = "bodies")

```

<br>

## Bigrams

Bigrams are two words that occur adjacent to each other in the text. Figure \@ref(fig:bigrams-part2) show the most common ***simple bigrams*** in each 5-year period. These are derived from the submission titles. Note the low number of occurrences; we cannot infer much given such low numbers.

"Grandma wind" is third bigram in the 2010-2014 column. It illustrates some of the challenges and limitations in doing this kind of textual analysis. It's part of the title of the thread "Don't throw away Grandma's wind-up desk clock" started by Danny Burstein in Volume 26.

* ***Grandma's*** was lemmatized to "grandma"
* ***wind-up*** was separated into two words
* ***up*** was filtered out, because it's a short, common word.

```{r bigrams-part2, fig.height=9, fig.width=9, fig.cap="Simple bigrams"}

do_top_words_plot(df = d_bigrams, 
                  gram = "bigrams",
                  which_plots = 1)
```

<br>

Figure \@ref(fig:bigram-noun-phrases-part2) presents the same visualization using ***noun phrase bigrams from titles***. Note the similarly low frequencies.

Here again we can see limitations: AT&T and fighter planes F15, F-16, and F22 should not be considered bigrams.

```{r bigram-noun-phrases-part2, fig.height=9, fig.width=9, fig.cap="Noun phrase bigrams from titles"}

do_top_words_plot(df = d_noun_phrases %>%
                    filter(n_words == 2), 
                  gram = "noun phrase bigrams",
                  which_plots = 1)
```

The most common bigrams sourced from ***bodytext noun phrases*** are shown in \@ref(fig:bigrams-noun-phrases-body-part2). Again place-related n-grams dominate the top of the list. This could be due in part to these bigrams being part of email signatures that I imperfectly filtered out. Further down the list we do see n-grams that we otherwise would miss, including ***college board***, ***Fannie Mae***, ***B-2***, ***Kapersky Lab***, ***Ashley Madison***, and ***Homeland Security***.

```{r bigrams-noun-phrases-body-part2, fig.height=9, fig.width=9, fig.cap="noun phrase bigrams from body text"}

do_top_words_plot(df = d_noun_phrases_body %>%
                    filter(n_words == 2,
                           issue_year <= 2019), 
                  gram = "bigrams",
                  which_plots = 1,
                  df_source = "bodies"
                  )
```

<br>

There is no one "correct" way to collect n-grams from a corpus. Using multiple techniques provides a richer view into the topics that have been addressed in the RISKS Forum.

<br>
<br>

---

By Daniel Moul (heydanielmoul via gmail)

![CC-BY](https://i.creativecommons.org/l/by/4.0/88x31.png) This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)
